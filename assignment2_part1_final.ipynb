{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96651e2b20b2f0f51f5cf5bf2682852b",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# Assignment 2 Part 1: Vector Space Model and Relevance Feedback (50 pts)\n",
    "\n",
    "In this assignment, we will improve our simple text retrieval system built in Assignment 1 by leveraging the Vector Space Model and relevance feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4b2b0df1d5506f8d631bb423bb42242",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignores warnings, should be on for submitting to the autograder but you may comment it out if needed while working.\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abe0afea14e08b7c8cc98dbe5cf6ed66",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Retrieve and rank documents with TF-IDF weighting scheme (10 pts)\n",
    "\n",
    "Modify your `retrieve_n_rank_docs` function from Assignment 1 so that it now uses the TF-IDF weights to retrieve and rank documents as shown in the lecture slides. Specifically, for each *term* in a given query, we accumulate the TF-IDF weight for each document that contains the term. We apply **maximum frequency normalisation** to raw term frequencies to obtain the TF and follow the definition of IDF given in the lecture. After repeating this procedure for all terms, we rank the documents found in descending order of their total TF-IDF weights **rounded to three decimal places**. Again, if two documents tie, the document with a *lower* document number should be ranked higher. Refer to the lecture slides titled \"Ranking Documents: Example\" in the TF-IDF slide deck for more details. \n",
    "\n",
    "Your modified function should accept the same set of arguments as in Assignment 1 and likewise output a `dict` that contains an ordered `list` of retrieved documents for each query, similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q1'  : ['d51', 'd486', ..., 'd876'], \n",
    "    'q2'  : ['d12', 'd51', ..., 'd486'],\n",
    "    ...,\n",
    "    'q225': ['d1188', 'd1380', ..., 'd173']\n",
    "}\n",
    "```\n",
    "\n",
    "⚠️ Please remember to **round the total TF-IDF weights to three decimal places** before ranking the documents. ⚠️\n",
    "\n",
    "**This function should return a `dict` of length `len(queries)`, where each key is a `str` representing a `q_id` and each value is a `list` that contains the `doc_id` of the retrieved documents. `max_docs` determines the maximum number of documents that can be retrieved for each query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cf2e4d47530928e8a669b757b5e573c",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def retrieve_n_rank_docs(inverted_index, queries, max_docs=-1):\n",
    "    \"\"\"\n",
    "    Retrieve documents in order of relevance from an inverted index based on some queries.\n",
    "    \"\"\"\n",
    "    \n",
    "        # Calculate maximum frequency for each document\n",
    "    max_freq = {}\n",
    "    for token, doc_data in inverted_index.items():\n",
    "        for doc_id, freq_tuple in doc_data.items():\n",
    "            freq = freq_tuple[0]  # Assuming the frequency is the first element in the tuple\n",
    "            if doc_id not in max_freq:\n",
    "                max_freq[doc_id] = freq\n",
    "            else:\n",
    "                max_freq[doc_id] = max(max_freq[doc_id], freq)\n",
    "\n",
    "    # Total count of documents\n",
    "    N = len({doc_key for doc_data in inverted_index.values() for doc_key in doc_data})\n",
    "    \n",
    "    ret_docs ={}\n",
    "    for q_id, terms in queries.items():\n",
    "        doc_scores = {}\n",
    "        for term in terms:\n",
    "            if term in inverted_index:\n",
    "                k = len(inverted_index[term])\n",
    "                idf = 1 + math.log(N/k)\n",
    "                for doc_id, freq_tuple in inverted_index[term].items():\n",
    "                    freq = freq_tuple[0]\n",
    "                    tf = 0.5 + (0.5 * freq / max_freq[doc_id])\n",
    "                    if doc_id not in doc_scores:\n",
    "                        doc_scores[doc_id] = 0\n",
    "                    doc_scores[doc_id] = doc_scores[doc_id] + (tf * idf)\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: (-round(x[1], 3), int(x[0][1:])))\n",
    "        # Select top documents based on max_docs\n",
    "        top_docs = [doc_id for doc_id, _ in sorted_docs][:max_docs if max_docs != -1 else None]\n",
    "        ret_docs[q_id] = top_docs\n",
    "\n",
    "    return ret_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2642d22420d606a15a6c8dd048ec2f85",
     "grade": false,
     "grade_id": "cell-3a147618f1739101",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import the reference implementations of some utility functions from Week 1 to be used in the autograder tests\n",
    "# They should be used ONLY in the autograder tests, NOT in your solutions\n",
    "# Ignore any ModuleNotFoundError message in the autograder feedback - it does not affect your grade\n",
    "from utils import load_cranfield_docs, load_cranfield_queries, build_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bab434baf829c084194d3536d37448c",
     "grade": true,
     "grade_id": "cell-5b892e216b90d5e7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The definitions of the three utility functions are hidden here - not for grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58b95a91c0a52889577fcbce14460f72",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "min_df = 10 # min_df won't change in the hidden tests\n",
    "stu_docs = load_cranfield_docs()\n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=min_df)\n",
    "stu_queries = load_cranfield_queries()\n",
    "\n",
    "max_docs = 10 # max_docs may vary in the hidden tests\n",
    "stu_ret_docs = retrieve_n_rank_docs(stu_inv_index, stu_queries, max_docs=max_docs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_ret_docs, dict), \"Q1: Your function should return a dictionary. \"\n",
    "assert len(stu_ret_docs) == len(stu_queries), \"Q1: Your dictionary should have the same length as there are queries. \"\n",
    "\n",
    "for q_id in stu_ret_docs:\n",
    "    \n",
    "    assert q_id in stu_queries, f\"Q1: When max_docs = {max_docs}, '{q_id}' in your dictionary is not a valid q_id. \"\n",
    "    \n",
    "    assert len(stu_ret_docs[q_id]) <= max_docs, f\"Q1: When max_docs = {max_docs}, your # retrieved docs ({len(stu_ret_docs[q_id])}) for {q_id} is bigger than max_docs. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, stu_queries, stu_ret_docs, min_df, max_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50d1c46db54f169aaf033446834187ab",
     "grade": false,
     "grade_id": "cell-d49d380527e43db4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Rocchio Relevance Feedback (40 pts)\n",
    "\n",
    "Now comes the really fun stuff. Let's try to simulate an interactive Rocchio relevance feedback loop. The two main characters in the story are:\n",
    "\n",
    "* an inquisitive `User` who never ceases issuing queries; and\n",
    "* a diligent `RetrievalSystem` that always attends to the queries with extraordinary patience.\n",
    "\n",
    "You will play the role of `RetrievalSystem`, represented by the `RetrievalSystem` class. An instance of the `User` class can be created as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16b8127dee45eb5f2ae00a836903210",
     "grade": false,
     "grade_id": "cell-aba654f8c6e343a4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import User\n",
    "test_user = User(random_state=0) # The User in the autograder tests uses a different seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f67b335399623eda0f237736b7eae7b4",
     "grade": false,
     "grade_id": "cell-42457b95d61516c2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 2a: Perform Latent Semantic Indexing (LSI) (10 pts)\n",
    "\n",
    "As the first step of constructing a `RetrievalSystem` class, let's first complete the `__init__` method. It accepts a collection of documents, `docs`, as returned by the `load_cranfield_docs` function, and an argument `num_concepts` indicating the number of concepts/topics to keep for LSI. In addition to the argument `min_df` with its usual meaning, the method also accepts three other arguments, `alpha`, `beta` and `gamma`, that correspond to the $\\alpha$, $\\beta$ and $\\gamma$ parameters in the Rocchio query update formula. \n",
    "\n",
    "A document-term matrix with TF-IDF weights, `doc_term_mat`, has been created for you. Your task at this step is to perform LSI on the document-term matrix and obtain the LSI-transformed vectors in the concept space for each document. The LSI-transformed vectors should be stored in the variable `self.doc_vecs` as a 2-D `np.ndarray` of the shape `(len(docs), num_concepts)`. You may (and probably need to) create additional class attributes to hold objects that may be shared with other class methods. Please use `random_state=42` whenever it is required. \n",
    "\n",
    "\n",
    "**This class method should fill the class attribute `self.doc_vecs` with a 2-D `np.ndarray` of the shape `(len(docs), num_concepts)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46ac99965af689c1aed001cde8f36e3d",
     "grade": false,
     "grade_id": "cell-38d49c4872fe96ab",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "class RetrievalSystem:\n",
    "    def __init__(self, docs, num_concepts, min_df=1, alpha=1.0, beta=0.75, gamma=0.15):\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        self.vec = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "        doc_term_mat = self.vec.fit_transform([\" \".join(docs[doc_id]) for doc_id in docs])\n",
    "        self.q_vecs = {}\n",
    "        svd = TruncatedSVD(n_components=num_concepts, random_state=42)\n",
    "        self.doc_vecs = svd.fit_transform(doc_term_mat)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "164ee680842874caf51034022d866b1b",
     "grade": true,
     "grade_id": "cell-ce481913f67a4bf0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "num_concepts, min_df = 100, 10 # These won't change in the hidden tests\n",
    "\n",
    "stu_ret_sys = RetrievalSystem(stu_docs, num_concepts, min_df)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_ret_sys.doc_vecs, np.ndarray), \"Q2a: The doc_vecs attribute of your RetrievalSystem should be a np.ndarray. \"\n",
    "assert stu_ret_sys.doc_vecs.shape == (len(stu_docs), num_concepts), \"Q2a: The doc_vecs attribute of your RetrievalSystem has an incorrect shape. \"\n",
    "assert np.issubdtype(stu_ret_sys.doc_vecs.dtype, np.floating), \"Q2a: The doc_vecs attribute of your RetrievalSystem should have a float dtype. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ret_sys, num_concepts, min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38f9dc1ea192e49047ecff90389ff90a",
     "grade": false,
     "grade_id": "cell-aefd31eaf2760cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 2b: Retrieve and rank documents in the concept space (10 pts)\n",
    "\n",
    "Next, let's complete the method `retrieve_n_rank_docs` for retrieving and ranking documents in the concept space generated by your LSI earlier. The method accepts the same arguments `queries` and `max_docs` as you have seen in other retrieval functions. The only novelty is that now we keep the LSI-transformed vectors for all the documents instead of an inverted index. \n",
    "\n",
    "The argument `queries` is a `dict` of queries as returned by the `load_cranfield_queries` function. However, it may contain an arbitrary subset of the Cranfield queries rather than necessarily all the Cranfield queries. In fact, you should not assume any knowledge about the possible queries included in `queries`, because what queries you will receive is at the discretion of the inquisitive `User`. Upon receiving `queries`, you should turn each **NEW** query into an LSI-tranformed vector, following what you have done with the documents; for queries that you have received before, fetch their vectors from the class attribute `self.q_vecs`. The reason why we need to treat new and old queries differently is that later we will perform Rocchio updates on some query vectors so that their final vector representations will differ from the LSI representations they started with. All query vectors start as an LSI vector but may end up differently depending on how often they are involved in the feedback loop. \n",
    "\n",
    "Once you have the correct vector representation for each query, retrieve documents in descending order of the cosine similarity between their vector representations. The maximum number of documents to retrieve for each query is again governed by the argument `max_docs`. As before, your method should finally output a `dict` containing the documents retrieved for each query, similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q217': ['d983', 'd554', ..., 'd623'],\n",
    "    'q99' : ['d716', 'd67', ..., 'd164'],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "It is fine if you see a `RuntimeWarning`. That's because two document vectors are zero vectors. \n",
    "\n",
    "\n",
    "⚠️ **Always remember, the document numbers start at 1.** Namely, the first document is `d1`. ⚠️\n",
    "\n",
    "**This method should return a `dict` of length `len(queries)`, where each key is a `str` representing a `q_id` and each value is a `list` that contains the `doc_id` of the retrieved documents in order. `max_docs` determines the maximum number of documents to retrieve for each query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbc32edd0fa85ff0c2f9a094488c269f",
     "grade": false,
     "grade_id": "cell-3f5bce7fe99fdc6b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class RetrievalSystem:\n",
    "    \n",
    "    def __init__(self, docs, num_concepts, min_df=1, alpha=1.0, beta=0.75, gamma=0.15):\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        self.vec = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "        doc_term_mat = self.vec.fit_transform([\" \".join(docs[doc_id]) for doc_id in docs])\n",
    "        self.q_vecs = {}\n",
    "        self.svd = TruncatedSVD(n_components=num_concepts, random_state=42)\n",
    "        self.doc_vecs = self.svd.fit_transform(doc_term_mat)\n",
    "\n",
    "\n",
    "    def retrieve_n_rank_docs(self, queries, max_docs=-1):\n",
    "        \"\"\"\n",
    "        Retrieve and rank documents in the latent semantic (concept) space\n",
    "        \"\"\"\n",
    "        \n",
    "        ret_docs = {}\n",
    "        for q_id, q_terms in queries.items():\n",
    "            if q_id not in self.q_vecs:\n",
    "                q_vec = self.vec.transform([\" \".join(q_terms)])\n",
    "                self.q_vecs[q_id] = self.svd.transform(q_vec)\n",
    "            cos_sim = []\n",
    "            for doc_vec in self.doc_vecs:\n",
    "                sim = np.dot(self.q_vecs[q_id], doc_vec.T)/(np.linalg.norm(self.q_vecs[q_id])*np.linalg.norm(doc_vec))\n",
    "                cos_sim.append(sim)\n",
    "            cos_sim = np.array(cos_sim).flatten()\n",
    "            ranked_docs = np.argsort(-cos_sim)[:max_docs if max_docs != -1 else None]\n",
    "            ret_docs[q_id] = [f'd{idx + 1}' for idx in ranked_docs]\n",
    "                \n",
    "        \n",
    "        return ret_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffe438913cba54d440a89341c8222cca",
     "grade": true,
     "grade_id": "cell-94b2a07cd98ffa25",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "num_concepts, min_df = 100, 10 # These won't change in the hidden tests\n",
    "max_docs = 10 # This may vary in the hidden tests\n",
    "\n",
    "queries = load_cranfield_queries()\n",
    "stu_queries = dict(random.sample(queries.items(), 100)) # Test on a random sample\n",
    "\n",
    "stu_ret_sys = RetrievalSystem(stu_docs, num_concepts, min_df)\n",
    "stu_ret_docs = stu_ret_sys.retrieve_n_rank_docs(stu_queries, max_docs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_ret_docs, dict), \"Q2b: Your method should return a dictionary. \"\n",
    "assert len(stu_ret_docs) == len(stu_queries), \"Q2b: Your dictionary should have the same length as there are queries. \"\n",
    "\n",
    "for q_id in stu_ret_docs:\n",
    "    \n",
    "    assert q_id in stu_queries, f\"Q2b: When max_docs = {max_docs}, '{q_id}' in your dictionary is not a valid q_id. \"\n",
    "    \n",
    "    assert len(stu_ret_docs[q_id]) <= max_docs, f\"Q2b: When max_docs = {max_docs}, your # retrieved docs ({len(stu_ret_docs[q_id])}) for {q_id} is bigger than max_docs. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ret_sys, num_concepts, min_df\n",
    "del max_docs, queries, stu_queries, stu_ret_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed5aea5b6823f2ffa76f6f727f25813e",
     "grade": false,
     "grade_id": "cell-1d52bb6b29966e5c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 2c: Perform Rocchio query updates (20 pts)\n",
    "\n",
    "Finally it's time to engage with the inquisitive `User`. Complete the method `gather_feedback` for inviting a `user` to your Rocchio relevance feedback loop. Your interaction with the `user` proceeds in four steps. \n",
    "\n",
    "* **Step 1**: You request the `user` to issue queries by invoking the `user`'s `issue_queries` method. The `user` then returns to you a `dict` of queries represented by `queries`, and a number `max_docs` indicating the maximum number of documents the `user` wants to retrieve for each query. \n",
    "\n",
    "\n",
    "* **Step 2**: You retrieve the required number of documents based on `queries` as you did in the last question. \n",
    "\n",
    "\n",
    "* **Step 3**: You invite the `user` to give feedback to the documents retrieved, `ret_docs`, by invoking the `user`'s `give_feedback` method with `ret_docs` passed along. Normally in a relevance feedback loop, the `user` would mark each document retrieved as either relevant or irrelevant; however, the `user` you are engaging with prefers giving feedback in the form of **precisions at each rank**. \n",
    "    \n",
    "    Specifically, suppose your `ret_docs` is as follows:\n",
    "\n",
    "    ```\n",
    "    {\n",
    "        'q167': ['d553', 'd1100', 'd1096', 'd1279', 'd1099'], \n",
    "        'q54' : ['d123', 'd554', 'd623', 'd398', 'd102'], \n",
    "        'q197': ['d768', 'd884', 'd883', 'd909', 'd882']\n",
    "    }\n",
    "    ```\n",
    "    from which we can deduce that the `user` must have issued three queries and for each query at most five documents should be retrieved. If you pass your `ret_docs` to the `user` for feedback, you would get back:\n",
    "    \n",
    "    ```\n",
    "    {\n",
    "        'q167': [0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "        'q54' : [0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "        'q197': [0.0, 0.5, 0.3333333333333333, 0.25, 0.2]\n",
    "    }\n",
    "    ```\n",
    "    where each number indicates the precision at that rank. For example, the second number `0.5` from `q197` indicates that the precision at rank 2 is `0.5` for query `q197`. Similarly, the precision at rank 4 is `0.25` for query `q197`. For queries `q167` and `q54`, the precisions at top 5 ranks are, unfortunately, zero. The precisions at each rank are returned back to you in the variable `pre_at_n`, along with another variable `avg_ndcg` which indicates your average NDCG for all queries. \n",
    "    \n",
    "Code for the first three steps has been provided to you. Your task is to complete the code for\n",
    "\n",
    "* **Step 4**: You perform a Rocchio update on all the queries the `user` issued to you based on the feedback you received. As a result, the query vectors in `self.q_vecs` should be updated accordingly. In other words, the \"modified query vector\" for each query under consideration should be stored in the corresponding entry in `self.q_vecs`. \n",
    "\n",
    "⚠️ **Always remember, the document numbers start at 1.** Namely, the first document is `d1`. ⚠️\n",
    "\n",
    "**This method should return a copy of `self.q_vecs` for grading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "332ca4ad8bfac2dcfd906cc15dcf3997",
     "grade": false,
     "grade_id": "cell-a8150e5145aa8eed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "class RetrievalSystem:\n",
    "    \n",
    "    def __init__(self, docs, num_concepts, min_df=1, alpha=1.0, beta=0.75, gamma=0.15):\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        self.vec = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "        doc_term_mat = self.vec.fit_transform([\" \".join(docs[doc_id]) for doc_id in docs])\n",
    "        self.q_vecs = {}\n",
    "        self.svd = TruncatedSVD(n_components=num_concepts, random_state=42)\n",
    "        self.doc_vecs = self.svd.fit_transform(doc_term_mat)\n",
    "\n",
    "\n",
    "    def retrieve_n_rank_docs(self, queries, max_docs=-1):\n",
    "        \"\"\"\n",
    "        Retrieve and rank documents in the latent semantic (concept) space\n",
    "        \"\"\"\n",
    "        \n",
    "        ret_docs = {}\n",
    "        for q_id, q_terms in queries.items():\n",
    "            if q_id not in self.q_vecs:\n",
    "                q_vec = self.vec.transform([\" \".join(q_terms)])\n",
    "                self.q_vecs[q_id] = self.svd.transform(q_vec)\n",
    "            cos_sim = []\n",
    "            for doc_vec in self.doc_vecs:\n",
    "                sim = np.dot(self.q_vecs[q_id], doc_vec.T)/(np.linalg.norm(self.q_vecs[q_id])*np.linalg.norm(doc_vec))\n",
    "                cos_sim.append(sim)\n",
    "            cos_sim = np.array(cos_sim).flatten()\n",
    "            ranked_docs = np.argsort(-cos_sim)[:max_docs if max_docs != -1 else None]\n",
    "            ret_docs[q_id] = [f'd{idx + 1}' for idx in ranked_docs]\n",
    "    \n",
    "        return ret_docs\n",
    "    \n",
    "\n",
    "    def gather_feedback(self, user):\n",
    "        \"\"\"\n",
    "        This function models the interactive relevance feedback loop.\n",
    "        \"\"\"\n",
    "        queries, max_docs = user.issue_queries()\n",
    "        ret_docs = self.retrieve_n_rank_docs(queries, max_docs=max_docs)\n",
    "        pre_at_n, avg_ndcg = user.give_feedback(ret_docs)\n",
    "        \n",
    "        for q_id in queries:\n",
    "            relevant_docs_indices = []\n",
    "            non_relevant_docs_indices = []\n",
    "            prev_precision_product = 0\n",
    "            rank = 1\n",
    "            for doc_id, precision in zip(ret_docs[q_id], pre_at_n[q_id]):\n",
    "                doc_index = int(doc_id[1:]) - 1\n",
    "                precision_product = rank * precision\n",
    "                if precision_product > prev_precision_product:\n",
    "                    relevant_docs_indices.append(doc_index)\n",
    "                else:\n",
    "                    non_relevant_docs_indices.append(doc_index)\n",
    "                prev_precision_product = precision_product\n",
    "                rank += 1\n",
    "            relevant_vectors = self.doc_vecs[relevant_docs_indices] if relevant_docs_indices else np.zeros(self.doc_vecs.shape[1])\n",
    "            non_relevant_vectors = self.doc_vecs[non_relevant_docs_indices] if non_relevant_docs_indices else np.zeros(self.doc_vecs.shape[1])\n",
    "            query_vec = self.q_vecs[q_id]\n",
    "            rel_component = np.mean(relevant_vectors, axis=0) if relevant_docs_indices else np.zeros(self.doc_vecs.shape[1])\n",
    "            non_rel_component = np.mean(non_relevant_vectors, axis=0) if non_relevant_docs_indices else np.zeros(self.doc_vecs.shape[1])\n",
    "            self.q_vecs[q_id] = self.alpha * query_vec + self.beta * rel_component - self.gamma * non_rel_component\n",
    "                \n",
    "\n",
    "        return self.q_vecs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "643bd520e7feee1bd29adf1fa98ab6b8",
     "grade": true,
     "grade_id": "cell-70974b1dcf8b7f3d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The definition of the User class is hidden here - not for grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a2070042e4573d774357336cad3a2b2",
     "grade": true,
     "grade_id": "cell-97527c3c993bc9a6",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "num_concepts, min_df = 100, 10 # These won't change in the hidden tests\n",
    "\n",
    "stu_ret_sys = RetrievalSystem(stu_docs, num_concepts, min_df)\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ret_sys, num_concepts, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
