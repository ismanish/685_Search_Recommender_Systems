{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9041f50a45825864dbda8abc1d8c2b32",
     "grade": false,
     "grade_id": "cell-4139388145c4585d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 2 Part 2: Query Log Analysis (50 pts)\n",
    "\n",
    "\n",
    "\n",
    "If you aspired to run a successful information retrieval system like a search engine, you would definitely like to know your users better so that you can better serve them (and spam them with more targeted ads). One important way of doing so is analysing what they have been searching for, namely, a query log. \n",
    "\n",
    "In this assignment we will analyse [a query log data set](https://github.com/microsoft/BingCoronavirusQuerySet) curated by Microsoft Bing Search. Data from April 2020 to March 2021 (inclusive) is provided under `assets/BingCoronavirusQuerySet`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e655c4dd076f9e41262640dabb95420",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Identify users from queries (30 pts)\n",
    "\n",
    "If you have a reasonable portrait of your users, then given an anonymous query you should be able to identify the user who issued that query with a reasonable accuracy. This is useful for \"privacy-aware\" personalisation where we still want to offer some personalisation even without any identifying information. We can effectively frame the problem of identifying users from queries as a *text classification* problem where the queries are the inputs and the users are the class labels. \n",
    "\n",
    "The Bing search log data set is completely de-identified, so we don't actually have access to the information about individual users. Nevertheless, we can think of each country as a \"user\" (or a homogenous group of individual users) and try to understand such a \"user\" better. In what follows, we will use the words \"country\" and \"user\" interchangeably. \n",
    "\n",
    "Your task for this question is to complete the `TextClassifier` class below which has three methods:\n",
    "\n",
    "* **`_load_data`**: It loads data from a `.tsv` file at the given `path` and returns a `pd.DataFrame` that is in the same format as the `.tsv` file (i.e., has the same column titles and order). **In this assignment, we only consider queries with a `PopularityScore` at least `min_pop_score` as \"queries of interest\" and countries from which at least `min_num_qs` number of \"queries of interest\" were issued as \"users of interest\".** We will only try to identify \"users of interest\" from \"queries of interest\", so other queries or users should be filtered out in your `pd.DataFrame`.\n",
    "\n",
    "\n",
    "* **`fit`**: It uses data from a `.tsv` file at the given `path` as training data to fit the classifier `self.clf`, subject to the same `min_pop_score` and `min_num_qs` constraints. **You may use any official `sklearn` classifiers there are or write your own classifiers,** as long as the model stored in `self.clf` is a `BaseEstimator` and passes the `is_classifier` check (don't need to worry about this if you use an official classifier). In addition, feel free to define any auxiliary attributes in `__init__` as you see fit. \n",
    "\n",
    "\n",
    "* **`get_test_X_y`**: It turns data from a `.tsv` file at the given `path` into test data in the form of `X` and `y` as what would be expected by the `score` method of a typical classifier, subject to the same `min_pop_score` and `min_num_qs` constraints. **The label vector `y` should only contain labels that are also in your training data and correspondingly, so should the data matrix `X`.** You are assured that any call to `get_test_X_y` is always preceded by a call to `fit`, so that you have access to the labels in your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f24f494b110648601a1fdd0446cce21",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self):\n",
    "        self.clf = None\n",
    "        self.clf = MultinomialNB()\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "        \n",
    "    def _load_data(self, path, min_pop_score=10, min_num_qs=300):\n",
    "        \"\"\"\n",
    "        Loads data into a DataFrame with the required filtering\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path, sep = '\\t')\n",
    "        df = df[df['PopularityScore'] >= min_pop_score]\n",
    "        country_counts = df['Country'].value_counts()\n",
    "        valid_countries = country_counts[country_counts >= min_num_qs].index\n",
    "        df = df[df['Country'].isin(valid_countries)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit(self, path, min_pop_score=10, min_num_qs=300):\n",
    "        \"\"\"\n",
    "        Fits the classifier using data from 'path'\n",
    "        \"\"\"\n",
    "        df = self._load_data(path, min_pop_score, min_num_qs)\n",
    "        X = self.vectorizer.fit_transform(df['Query'])\n",
    "        self.clf.fit(X, df['Country'])\n",
    "        return self.clf\n",
    "    \n",
    "    def get_test_X_y(self, path, min_pop_score=10, min_num_qs=300):\n",
    "        \"\"\"\n",
    "        Generates test data from 'path'\n",
    "        \"\"\"\n",
    "        df = self._load_data(path, min_pop_score, min_num_qs)\n",
    "        df = df[df['Country'].isin(self.clf.classes_)]\n",
    "        X = self.vectorizer.transform(df['Query'])\n",
    "        y = df['Country']\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75cc47277aa4b42205e9b5ad4660999c",
     "grade": false,
     "grade_id": "cell-170fc31aa775eecf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your `_load_data` method will be graded as usual by comparison with the correct answer. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe496347df6a7f8885005278caef0c70",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests for loading data\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# This won't change in the hidden test. \n",
    "path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-04-01_2020-04-30.tsv\" \n",
    "\n",
    "# These may vary. \n",
    "min_pop_score, min_num_qs = 10, 300\n",
    "\n",
    "stu_text_clf = TextClassifier()\n",
    "stu_df = stu_text_clf._load_data(path, min_pop_score, min_num_qs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_df, pd.DataFrame), \"Q1: Your function should return a pd.DataFrame. \"\n",
    "assert stu_df[\"PopularityScore\"].min() >= min_pop_score, f\"Q1: Some queries in your pd.DataFrame have a PopularityScore < min_pop_score = {min_pop_score}. \"\n",
    "assert len(stu_df) >= len(pd.unique(stu_df[\"Country\"])) * min_num_qs, f\"Q1: Some countries in your pd.DataFrame have less than min_num_qs = {min_num_qs} queries. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_text_clf, stu_df, min_pop_score, min_num_qs, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d528f1c8d062004659189d8f8500943a",
     "grade": false,
     "grade_id": "cell-e833ffd28bc507f5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your `fit` and `get_test_X_y` methods will be graded by comparison with a simple baseline text classifier. Specifically, we will take each month's data as the training data and evaluate your text classifier on the test data generated from the following month. An example is given in the visible tests below, where we fit your text classifier on the data from April 2020 and use it to generate test data for May 2020. The autograder will compute the Matthews correlation coefficient as a performance measure and determine whether your text classifier outperforms the baseline. **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7804ad892c557bf707d17db28d5c48e0",
     "grade": true,
     "grade_id": "cell-ce481913f67a4bf0",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests for text classification\n",
    "from sklearn.base import BaseEstimator, is_classifier\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# These won't change in the hidden tests \n",
    "min_pop_score, min_num_qs = 10, 300\n",
    "\n",
    "# These may vary.\n",
    "train_path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-04-01_2020-04-30.tsv\"\n",
    "test_path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-05-01_2020-05-31.tsv\"\n",
    "\n",
    "# Define a text classfier and fit it with training data\n",
    "stu_text_clf = TextClassifier()\n",
    "stu_text_clf.fit(train_path, min_pop_score, min_num_qs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert hasattr(stu_text_clf, \"clf\"), \"Q1: Your text classifier should have an attribute 'clf'. \"\n",
    "assert isinstance(stu_text_clf.clf, BaseEstimator), \"Q1: Your clf should be a sklearn Estimator. \"\n",
    "assert is_classifier(stu_text_clf.clf), \"Q1: Your clf should be a sklearn Classifier. \"\n",
    "assert hasattr(stu_text_clf.clf, \"predict\"), \"Q1: Your clf should have a 'predict' method. \"\n",
    "\n",
    "# Generate test data\n",
    "stu_X, stu_y = stu_text_clf.get_test_X_y(test_path, min_pop_score, min_num_qs)\n",
    "\n",
    "# check that X and y are valid\n",
    "check_X_y(stu_X, stu_y, accept_sparse=True)\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_text_clf, stu_X, stu_y, min_pop_score, min_num_qs, train_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8751f4816865575437bb85afc4361ac2",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Detect query drifts (10 pts)\n",
    "\n",
    "In the previous question, we take a month's query logs as training data and use the query logs in the following month as test data to evaluate the performance of our text classifier (and in some sense how well we know about our users). This is under the assumption that queries issued by the same user are \"similar\" between two consecutive months. But to what extent is this assumption valid? Can there be a \"query drift\" over months? \n",
    "\n",
    "Let's try to detect possible query drifts using similarity measures. Complete the function below that computes the average cosine similarity between all pairs of queries issued by a user. Your function should first load the training data from `train_path` and the test data from `test_path` like you did in the previous question, subject to the same `min_pop_score` and `min_num_qs` constraints. Then it should fit a default `TfidfVectorizer` on the training data for converting queries into vectors. Next, **for countries present in both training and test data**, compute the average cosine similarity between all pairs of queries issued by each country. **Each pair consists of one query from the training data and the other from the test data.** If, for each country, you imagine the queries in the training and test data as forming two clusters respectively, then the similarity measure you are computing is also known as \"average linkage\". If the two clusters turn out to be \"dissimilar\", then there has probably been a query drift. \n",
    "\n",
    "Your function should output a `dict` that contains the above-mentioned average cosine similarity for each country, similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'Brazil': 0.1176541008844407,\n",
    "    'United States': 0.0774657184345132,\n",
    "    ...,\n",
    "    'China': 0.15424835919750043\n",
    "}\n",
    "```\n",
    "\n",
    "**Hint:** the vectors produced by a `TfidfVectorizer` are already normalised to unit length, which should greatly simplify the calculation of cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e472c4bccbd5d4804cdc4d9e71981007",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_avg_cos_sim(train_path, test_path, min_pop_score=10, min_num_qs=300):\n",
    "    \n",
    "    text_classifier = TextClassifier()\n",
    "    \n",
    "    train_df = text_classifier._load_data(train_path, min_pop_score, min_num_qs)\n",
    "    test_df = text_classifier._load_data(test_path, min_pop_score, min_num_qs)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vectors = vectorizer.fit_transform(train_df['Query'])\n",
    "    test_vectors = vectorizer.transform(test_df['Query'])\n",
    "    \n",
    "    avg_cos_sim = {}\n",
    "    for country in set(train_df['Country']).intersection(test_df['Country']):\n",
    "        train_country_vectors = train_vectors[train_df['Country'] == country]\n",
    "        test_country_vectors = test_vectors[test_df['Country'] == country]\n",
    "        if isinstance(train_country_vectors, csr_matrix):\n",
    "            # Convert to a dense matrix if sparse, for dot product computation\n",
    "            train_country_vectors = train_country_vectors.toarray()\n",
    "        if isinstance(test_country_vectors, csr_matrix):\n",
    "            test_country_vectors = test_country_vectors.toarray()\n",
    "            \n",
    "        cos_sim_matrix = np.dot(train_country_vectors, test_country_vectors.T)\n",
    "        avg_cos_sim[country] = np.mean(cos_sim_matrix) if cos_sim_matrix.size else 0\n",
    "        \n",
    "\n",
    "    return avg_cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49866a10cdca1c42ce3ee5296de4f994",
     "grade": true,
     "grade_id": "cell-94b2a07cd98ffa25",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "import math\n",
    "\n",
    "# These won't change in the hidden tests \n",
    "min_pop_score, min_num_qs = 10, 300\n",
    "\n",
    "# These may vary.\n",
    "train_path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-04-01_2020-04-30.tsv\"\n",
    "test_path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-05-01_2020-05-31.tsv\"\n",
    "\n",
    "stu_avg_cos_sim = compute_avg_cos_sim(train_path, test_path, min_pop_score, min_num_qs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_avg_cos_sim, dict), \"Q2: Your function should output a dictionary. \"\n",
    "assert all([isinstance(cos_sim, float) for cos_sim in stu_avg_cos_sim.values()]), \"Q2: All values of your dictionary should be Python floats. \"\n",
    "assert all([0 <= cos_sim <= 1 for cos_sim in stu_avg_cos_sim.values()]), \"Q2: All cosine similarity must be between 0 and 1. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_avg_cos_sim, min_pop_score, min_num_qs, train_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6978ff738a719a391f15bce8b14e4eb9",
     "grade": false,
     "grade_id": "cell-d49d380527e43db4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3: Group users by queries (10 pts)\n",
    "\n",
    "So far our analysis has been focused on queries issued by the same user. A user-centric analysis is also possible; for example, we could try to identify groups of users who have issued \"similar\" queries in a given month. Those users in the same group could share similar information needs. \n",
    "\n",
    "Complete the function below for clustering users into such groups. Like before, your function should first load a `.tsv` file from the given `path` subject to the same `min_pop_score` and `min_num_qs` constraints. Use a default `TfidfVectorizer` for converting queries into vectors. We will represent each user as the average vector of all the queries issued by that user in the given month. Then apply a default `KMeans` with `n_clusters=num_clusters` and `random_state=42` to the **user vectors**. Finally, your function should output a `dict` showing the cluster membership, similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'Cluster 1': ['India', 'Australia', ..., 'China'],\n",
    "    'Cluster 0': ['United Kingdom', 'United States', ..., 'France'],\n",
    "    'Cluster 2': ['Mexico', 'Argentina']\n",
    "}\n",
    "```\n",
    "\n",
    "The cluster labels (e.g., `'Cluster 1'`) and the countries within each cluster can be in any order. \n",
    "\n",
    "⚠️ Your input to `KMeans` would be a $N\\times D$ matrix where each row stores the vector representation for each user. **Please make sure the rows are ordered in the same way as how the countries are ordered in the `.tsv` file.** In other words, please do not (inadvertently) sort the countries in any way. Use `sort=False` option if you want to use `df.groupby`. You may find `pd.unique` useful. This is because `KMeans` is sensitive to the input order. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41e76cc3c1d105bc1b0e0d8158ccdcb5",
     "grade": false,
     "grade_id": "cell-38d49c4872fe96ab",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_users(path, num_clusters, min_pop_score=10, min_num_qs=300):\n",
    "    \n",
    "    text_classifier = TextClassifier()\n",
    "    df = text_classifier._load_data(path, min_pop_score, min_num_qs)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    query_vectors = vectorizer.fit_transform(df['Query'])\n",
    "    \n",
    "    vector_df = pd.DataFrame(query_vectors.toarray())\n",
    "    vector_df['Country'] = df['Country'].values\n",
    "    \n",
    "    avg_vectors = vector_df.groupby('Country', sort=False).mean()\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(avg_vectors)\n",
    "    \n",
    "    user_clusters = {}\n",
    "    for idx, label in enumerate(kmeans.labels_):\n",
    "        cluster_label = 'Cluster {}'.format(label)\n",
    "        if cluster_label not in user_clusters:\n",
    "            user_clusters[cluster_label] = []\n",
    "        user_clusters[cluster_label].append(avg_vectors.index[idx])\n",
    "\n",
    "    return user_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad7f7d6ce6edca86588134c56904c95b",
     "grade": true,
     "grade_id": "cell-97527c3c993bc9a6",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "# These won't change in the hidden tests \n",
    "min_pop_score, min_num_qs = 10, 300\n",
    "\n",
    "# These may vary.\n",
    "path = \"assets/BingCoronavirusQuerySet/QueriesByCountry_2020-04-01_2020-04-30.tsv\"\n",
    "num_clusters = 3\n",
    "\n",
    "stu_user_clusters = cluster_users(path, num_clusters, min_pop_score, min_num_qs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_user_clusters, dict), \"Q3: Your function should output a dictionary. \"\n",
    "assert len(stu_user_clusters) == num_clusters, \"Q3: The length of your dictionary should be the same as num_clusters. \"\n",
    "\n",
    "stu_countries = [cty for v in stu_user_clusters.values() for cty in v]\n",
    "assert len(set(stu_countries)) == len(stu_countries), \"Q3: Some countries belong to more than one cluster. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_user_clusters, min_pop_score, min_num_qs, stu_countries, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
