{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1525242612912b9f5e96d6e9d23d6cfd",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 3: Adaptive Information Filtering (50 pts)\n",
    "\n",
    "In the previous two assignments, we have been building information retrieval systems that address a user's ad-hoc information needs expressed in a set of queries. Now we will attend to a user's long-term information needs by actively \"pushing\" information of interest to the user through Adaptive Information Filtering. Let's pretend we are running a newsfeed business that delivers [BBC news](http://mlg.ucd.ie/datasets/bbc.html) of five topics (business, entertainment, politics, sport and tech) to subscribers. Our goal is to infer a subscriber's interest from the feedback we receive and to deliver news of the right topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e18ee1b32e848379a0dd9715dfcc3d2b",
     "grade": false,
     "grade_id": "cell-02a587800a89d459",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Below is the definition of the `BBCNewsfeed` class that we will use to model a newsfeed. It is very similar to the `TwitterStream` class you worked with in Assignment 4 of `SIADS 632 Data Mining II`. Both classes are implemented as a Python `iterable` that can be used in a `for`-loop. You may refer to Assignment 4 of 632 for more examples about working with `iterable`s. At each iteration, an instance of this iterable `BBCNewsfeed` class returns\n",
    "\n",
    "* a row vector in the form of a [CSR sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) of shape `(1, 9635)` representing a news article; and \n",
    "\n",
    "* an integer from `0-4` (inclusive) representing the topic of the news article. \n",
    "\n",
    "A mapping from integers to topics is given as the class variable `ids_to_topics`. An example of retrieving the first news article is also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2475ee3e3b39c7d7ef7be5abd993be8",
     "grade": false,
     "grade_id": "cell-38ba62d8ac29243f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BBCNewsfeed:\n",
    "    \n",
    "    ids_to_topics = {0: \"business\", 1: \"entertainment\", 2: \"politics\", 3: \"sport\", 4: \"tech\"}\n",
    "\n",
    "    def __init__(self, data_path, random_state=None):\n",
    "        self._random = random.Random(random_state)\n",
    "        \n",
    "        # load data\n",
    "        tf = scipy.io.mmread(os.path.join(data_path, \"bbc.mtx\")).T.tocsr()  # (n_docs, n_terms)\n",
    "        df = np.nansum(tf / tf, axis=0)\n",
    "        self._data = np.log1p(tf).multiply(1 + math.log(tf.shape[0]) - np.log(df)).tocsr() # tf-idf\n",
    "        self.shape = self._data.shape\n",
    "        \n",
    "        # load labels\n",
    "        classes = np.genfromtxt(os.path.join(data_path, \"bbc.classes\"), dtype=int, comments=\"%\")\n",
    "        self._classes = classes[self._random.sample(range(classes.shape[0]), classes.shape[0])]\n",
    "        \n",
    "        self._curr_row = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.reset()\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._curr_row < self._classes.shape[0]:\n",
    "            idx, cls = self._classes[self._curr_row]\n",
    "            self._curr_row += 1\n",
    "            return self._data[idx], cls\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def reset(self):\n",
    "        self._curr_row = 0\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b153fcebd26739dadfa948d1c8793c16",
     "grade": false,
     "grade_id": "cell-7ede51c06f0dbd45",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x9635 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 122 stored elements in Compressed Sparse Row format>,\n",
       " 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab the first news article\n",
    "bbc_newsfeed = BBCNewsfeed(\"assets/bbc\", random_state=42)\n",
    "next(bbc_newsfeed) # returns a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a0ac56461810ff581ea099e6ec73c3f",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As discussed in the lecture, we can build an adaptive information filter using a binary classifier that determines whether a news article is of interest to the subscriber. After we recommend some news articles to the subscriber, the classifier is then trained on the subscriber's relevance feedback on our recommendations. The same back-and-forth interactions repeat until our newsfeed business goes bankrupt with no more news articles to deliver. \n",
    "\n",
    "Your task for this assignment is to complete the `AdapInfoFilter` class below that implements the said adaptive information filter. More specifically, you will need to complete the following two methods which act sequentially during each round of interaction.\n",
    "\n",
    "* **`do_filtering`**: This method takes in a `newsfeed` as defined above and steps into the newsfeed to receive the vector representation of a news article, `news_vec`, and the topic of the news article, `topic`, at every iteration. Whenever it has amassed a batch of `batch_sz` news articles and topics, it should use its classifier `self.clf` to select news articles of interest and the corresponding topics, and `yield` them as a `tuple` (e.g., `yield news_vecs, topics`, where `news_vecs` is a 2D CSR sparse matrix of shape `(N, 9635)` and `topics` is a `np.ndarray` of shape `(N,)`. And `N` is dynamically determined by the classifier. ) This process bears some resemblance to the `LossyCounter` you have implemented in 632, where you perform lossy counting only after a full *bucket* of tweets have arrived. It is OK if your `clf` selects zero news articles, in which case you will just `yield` a degenerate 2D CSR sparse matrix of shape `(0, 9635)` and a degenerate `np.ndarray` of shape `(0,)`; as a result, though, you won't receive any feedback during that round of interaction. \n",
    "\n",
    "\n",
    "* **`gather_feedback`**: The `feedback` from the subscriber is passed to this method as a binary `np.ndarray` of shape `(N,)` with `1` indicating relevance and `0` otherwise, along with the news vectors `news_vecs` and the topics `topics` that were `yield`-ed during the current round of interaction. You should then train your `clf` *incrementally* using `feedback`. You may use any classifier of your choice, although only a few classifiers in `sklearn` support the `partial_fit` method that allows online, incremental learning. It is also possible to append `topics` as an extra column to `news_vecs` to create new training data. \n",
    "\n",
    "\n",
    "You may have wondered, if the very first batch of news articles are selected *before* your `clf` ever receives any feedback to start learning, how the first batch should be selected. Our solution to this \"chicken-or-egg\" dilemma is that **we don't do any filtering on the first batch** --- we let it go in order to receive some initial feedback; but from the second batch onwards your `clf` should start to do its job. This is equivalent to providing you with a few initial relevant news articles to initialise your `clf`, a way of dealing with cold start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1edf359c137868d8824504a26d8c7fdb",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron\n",
    "\n",
    "\n",
    "class AdapInfoFilter:\n",
    "    \n",
    "    def __init__(self, random_state=None):\n",
    "        # Initialize with None, will be created after first batch\n",
    "        self.clf = None\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def do_filtering(self, newsfeed, batch_sz=100):\n",
    "        news_vecs, topics = [], []\n",
    "        for idx, (news_vec,topic) in enumerate(newsfeed):\n",
    "            news_vecs.append(news_vec)\n",
    "            topics.append(topic)\n",
    "            \n",
    "            if len(news_vecs) == batch_sz:\n",
    "                news_vecs_array = scipy.sparse.vstack(news_vecs)\n",
    "                topics_array = np.array(topics)\n",
    "                \n",
    "                if self.clf:\n",
    "                    predicted_interest = self.clf.predict(news_vecs_array)\n",
    "                    selected_news = news_vecs_array[predicted_interest == 1]\n",
    "                    selected_topics = topics_array[predicted_interest == 1]\n",
    "                    yield selected_news, selected_topics\n",
    "                else:\n",
    "                    yield news_vecs_array, topics_array\n",
    "                news_vecs, topics = [], []\n",
    "\n",
    "\n",
    "    def gather_feedback(self, feedback, news_vecs, topics):\n",
    "        if feedback.size > 0 & news_vecs.shape[0] > 0:\n",
    "            if self.clf is None:\n",
    "                self.clf = PassiveAggressiveClassifier(loss='log', random_state=self.random_state)\n",
    "                self.clf.partial_fit(news_vecs, feedback, classes=np.array([0, 1]))\n",
    "            else:\n",
    "                self.clf.partial_fit(news_vecs, feedback,classes=np.array([0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08729c07aa28104ba8f8bbd8db52243f",
     "grade": false,
     "grade_id": "cell-d77248f90f58ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Behind the scene is there a `Subscriber` class whose definition is hidden. But after an ordinary `import`, you can create an instance of the `Subscriber` class to play with. The `give_feedback` method of the `Subscriber` class is the most relevant to you, from which your `AdapInfoFilter` will receive feedback. An illustration of how an `Subscriber` interacts with an `AdapInfoFilter` is provided in the visible tests below. You don't need to worry too much about the details of the function calls, as long as your `do_filtering` and `gather_feedback` methods do what they are supposed to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ae61362cfdd9ab2338a0a0cb3d0104b",
     "grade": false,
     "grade_id": "cell-bc86a5d5efaff137",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import Subscriber\n",
    "\n",
    "# create a new instance to play with\n",
    "subscriber = Subscriber(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2cc9a06f6dce6883532b6e7648ea2f5",
     "grade": true,
     "grade_id": "cell-094b1740df0ecc49",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The definition of Subscriber is hidden here - 0 pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba6cb5a0abb8e783e6f92d8b7238742a",
     "grade": true,
     "grade_id": "cell-0d4fa6e6c10761ca",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Autograder tests - check do_filtering and gather_feedback (30 pts)\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "# These won't change in the hidden tests\n",
    "random_state, batch_sz = 42, 100\n",
    "\n",
    "bbc_newsfeed = BBCNewsfeed(\"assets/bbc\", random_state)\n",
    "stu_filter = AdapInfoFilter(random_state)\n",
    "\n",
    "subscriber = Subscriber(random_state=random_state)\n",
    "\n",
    "# Some sanity checks\n",
    "count = 0\n",
    "for idx, stu_batch in enumerate(stu_filter.do_filtering(bbc_newsfeed, batch_sz=batch_sz)):\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    assert isinstance(stu_batch, tuple), f\"Q1: For batch at index {idx}, your 'do_filtering' function should yield a tuple. \"\n",
    "    assert len(stu_batch) == 2, f\"Q1: For batch at index {idx}, the tuple yielded is of an incorrect length. \"\n",
    "    \n",
    "    # check news_vecs\n",
    "    assert scipy.sparse.isspmatrix_csr(stu_batch[0]) or isinstance(stu_batch[0], np.ndarray), f\"Q1: For batch at index {idx}, your news_vecs must be either a csr_matrix or a ndarray. \"\n",
    "    assert stu_batch[0].ndim == 2, f\"Q1: For batch at index {idx}, your news_vecs must be a 2D array (sparse or dense). \"\n",
    "    assert stu_batch[0].shape[0] <= batch_sz, f\"Q1: For batch at index {idx}, your news_vecs has more rows than batch_sz = {batch_sz}. \"\n",
    "    assert stu_batch[0].shape[1] == bbc_newsfeed.shape[1], f\"Q1: For batch at index {idx}, your news_vecs has an incorrect # columns. \"\n",
    "\n",
    "    # check topics\n",
    "    assert isinstance(stu_batch[1], np.ndarray), f\"Q1: For batch at index {idx}, your topics must be a np.ndarray. \"\n",
    "    assert stu_batch[1].ndim == 1, f\"Q1: For batch at index {idx}, your topics must be a 1D np.ndarray. \"\n",
    "    assert stu_batch[1].shape[0] <= batch_sz, f\"Q1: For batch at index {idx}, your topics has more elements than batch_sz = {batch_sz}. \"\n",
    "\n",
    "    assert stu_batch[0].shape[0] == stu_batch[1].shape[0], f\"Q1: For batch at index {idx}, the # rows of your news_vecs should equal to the # elements in your topics. \"\n",
    "    \n",
    "    ######################################\n",
    "    # The real interactions happen below #\n",
    "    ######################################\n",
    "    \n",
    "    # Step 1: Your filter recommends some news articles and topics\n",
    "    stu_news_vecs, stu_topics = stu_batch\n",
    "    \n",
    "    # Step 2: The subscriber gives feedback on the same news articles \n",
    "    feedback = subscriber.give_feedback(stu_news_vecs, stu_topics)\n",
    "    \n",
    "    # Step 3: Your filter gathers the feedback and updates its clf\n",
    "    stu_filter.gather_feedback(feedback, stu_news_vecs, stu_topics)\n",
    "\n",
    "\n",
    "assert count >= bbc_newsfeed.shape[0] // batch_sz, f\"Q1: Your do_filtering function should at least yield {bbc_newsfeed.shape[0] // batch_sz} times. \"\n",
    "\n",
    "\n",
    "# Re-define variables to be used in hidden tests\n",
    "bbc_newsfeed = BBCNewsfeed(\"assets/bbc\", random_state)\n",
    "stu_filter = AdapInfoFilter(random_state)\n",
    "# A different subscriber will be used\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del random_state, batch_sz, bbc_newsfeed, subscriber, stu_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4a2fb213907ce330dd6686bae7064b6",
     "grade": false,
     "grade_id": "cell-86e928bb412fa37e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your `AdapInfoFilter` will also be graded on the *utility* it achieves during the entire course of interaction. Whenever a recommended news article receives a 👍 from the subscriber, your `AdapInfoFilter` gets a utility of `+3`; otherwise, it gets a utility of `-1` (to prevent your `AdapInfoFilter` from overwhelming the subscriber with all the news articles indiscriminately). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a655481fb74e905912a73bc62cb7f4ce",
     "grade": true,
     "grade_id": "cell-727b52baf225ea11",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autograder tests - check if utility >= -40 (10 pts)\n",
    "\n",
    "# These won't change in the hidden tests\n",
    "random_state, batch_sz = 42, 100\n",
    "util_pos, util_neg = 3, -1\n",
    "\n",
    "bbc_newsfeed = BBCNewsfeed(\"assets/bbc\", random_state)\n",
    "stu_filter = AdapInfoFilter(random_state)\n",
    "\n",
    "# Some hidden tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfe006a2b5879072105278d66cfc3674",
     "grade": true,
     "grade_id": "cell-ccdcf77d66562cbc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autograder tests - check if utility >= -20 (10 pts)\n",
    "\n",
    "# These won't change in the hidden tests\n",
    "random_state, batch_sz = 42, 100\n",
    "util_pos, util_neg = 3, -1\n",
    "\n",
    "bbc_newsfeed = BBCNewsfeed(\"assets/bbc\", random_state)\n",
    "stu_filter = AdapInfoFilter(random_state)\n",
    "\n",
    "# Some hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2392a019d1e636b43f16c6696601ef91",
     "grade": false,
     "grade_id": "cell-943266b1a410d1ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## References for data used\n",
    "\n",
    "Greene, D., & Cunningham, P. (2006). Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering. In Proceedings of the 23rd International Conference on Machine Learning (pp. 377–384). Association for Computing Machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
