{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b3fa1d60c8c974c51ab9393e6af6d87",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1: Text Retrieval and Evaluation (100 pts)\n",
    "\n",
    "In this assignment, we will build a miniature text retrieval system and evaluate its efficacy using standard metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d4774c74510dc770d4ee3a4066caa0a",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Load the data set (30 pts)\n",
    "\n",
    "We will use a pre-processed version of the classic \"[Cranfield collections](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/)\" as our data set for implementing a text retrieval system. The Cranfield collections contain 1,400 short documents from the aerodynamics field, along with a set of 225 queries and the corresponding relevance judgements (that is, what documents are deemed relevant to what queries by human experts). In what follows, we will first create three functions for loading the three basic components (documents, queries and relevance judgements), respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bff1df1b305d4376ad967009e742531",
     "grade": false,
     "grade_id": "cell-d77248f90f58ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 1a:  Load the documents (10 pts)\n",
    "\n",
    "Complete the function below to load all Cranfield documents. Each of the 1,400 documents is stored as a single text file under `assets/cranfield/cranfieldDocs`. While the general file-reading procedures certainly work here, if you examine a sample file you will probably notice that the format of the content bears close resemblance to that of an HTML/XML file, in that it contains multiple \"fields\" demarcated by pairs of \"tags\" like `<TEXT>` and `</TEXT>`. This allows the use of XML-parsing tools in Python, such as the built-in [`xml.etree.ElementTree`](https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree) or external packages like [`lxml`](https://lxml.de/), which may be more useful than reading the files line by line. \n",
    "\n",
    "Regardless of the tools you will be using, we only consider anything in between `<TEXT>` and `</TEXT>` as the \"content\" of a document. We actually also need the identifier in between `<DOCNO>` and `</DOCNO>` to uniquely identify each document, but it is just a serial number from 1 to 1400. You can either grab that number or just keep a counter as you parse the documents in order. Once you grab the text in between `<TEXT>` and `</TEXT>` for each document, perform the following processing steps:\n",
    "\n",
    "* **Tokenise the text.** For no good reason, just use `word_tokenize` from the `nltk` library to perform a quick tokenisation. \n",
    "\n",
    "\n",
    "* **Remove stop words.** Words like \"a\", \"for\", \"the\" etc. appear in almost every document and therefore, do not help distinguish one document from another. We remove them to drastically reduce the size of the vocabulary we have to deal with. The `stopwords` utility from `nltk.corpus` can provide us with a list of stop words in English. \n",
    "\n",
    "\n",
    "* **Stem each word.** Words sharing a common stem usually have related meanings, for example, \"compute\", \"computer\" and \"computation\". As far as understanding the main idea of a document is concerned (so that it can be accurately retrieved given a relevant query), it is arguably not very useful to distinguish such words. So we can further shrink our vocabulary by keeping only the word stems. We will use `PorterStemmer` from `nltk.stem.porter` for this purpose. \n",
    "\n",
    "Now that each document is comprised of a sequence of tokens, we store all documents in a `dict` indexed by the `doc_id`s, similar to the following:\n",
    "\n",
    "```\n",
    "{\n",
    "    'd1'   : ['experiment', 'studi', ..., 'configur', 'experi', '.'], \n",
    "    'd2'   : ['studi', 'high-spe', ..., 'steadi', 'flow', '.'],\n",
    "    ..., \n",
    "    'd1400': ['report', 'extens', ..., 'graphic', 'form', '.']\n",
    "}\n",
    "```\n",
    "\n",
    "where we associate each document with a `doc_id` created by prefixing the document number found in between `<DOCNO>` and `</DOCNO>` by the letter \"d\". Even though in general we shouldn't rely on the order of the keys in a `dict`, **in this case please make sure the `doc_id`s are ordered as shown above**. In Python 3.7 or higher, a `dict` preserves insertion order, so as long as you add each document to your `dict` in ascending order of the document numbers, you should be good. This obviates the need for sorting when we need all documents in a nested list in a later assignment. \n",
    "\n",
    "\n",
    "**This function should return a `dict` of length 1400, where each key is a `str` representing a `doc_id` and each value is a `list` that contains the tokens resulting from processing a document as described above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the stopwords dataset from NLTK\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0285322e52213309748ac11e5d625cff",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_cranfield_docs():\n",
    "    \"\"\"\n",
    "    Load and process cranfield documents\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    doc = {}\n",
    "\n",
    "    for root_folder, dir, files in os.walk('assets/cranfield/cranfieldDocs'):\n",
    "        for filename in files:\n",
    "            if filename[-4:].isdigit():\n",
    "                file_path = os.path.join(root_folder, filename)\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                toks = word_tokenize(root.find('TEXT').text.strip())\n",
    "                doc['d' + root.find('DOCNO').text.strip()] = [stemmer.stem(tok) for tok in toks if tok not in stop_words]\n",
    "    doc = dict(sorted(doc.items(), key = lambda item: int(item[0][1:])))\n",
    "    \n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea50ae747e3671a9c0a1b98b6e21f93d",
     "grade": true,
     "grade_id": "cell-0d4fa6e6c10761ca",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_docs = load_cranfield_docs()\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_docs, dict), \"Q1a: Your function should return a dictionary. \"\n",
    "assert len(stu_docs) == 1400, \"Q1a: Your dictionary should contain 1,400 documents. \"\n",
    "\n",
    "for idx, doc_id in enumerate(stu_docs, 1):\n",
    "    assert f\"d{idx}\" == doc_id, f\"Q1a: {doc_id} was not inserted in the correct order. \"\n",
    "\n",
    "# Some hidden tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stu_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4077181662f7f2595fa4b9fe3854cfb",
     "grade": false,
     "grade_id": "cell-57e0e54cee2e9264",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 1b: Load the queries (10 pts)\n",
    "\n",
    "Next, let's create a function for loading the Cranfield queries. The queries are all stored in a single text file,  `assets/cranfield/cranfield.queries`. It is not hard to tell that each line contains a query number and a piece of query text, separated by the first whitespace. \n",
    "\n",
    "We will identify each query in a way similar to how we identify each document. Each query is assigned a `q_id` formed by prefixing the query number found on the beginning of each line with the letter \"q\". We will also need to perform the same tokenisation, stop words removal and word stemming on each piece of query text as we have done on the documents (why?). Finally, we will drop the **ending** period (\".\") that is a **stand-alone** token at the end of every query. This helps prevent a query from being \"superficially matched\" to a document purely on the basis of two matching periods. However, we shall **not** strip off ending periods that are part of a token (e.g., \"15.4**.**\") or periods that occur in the middle of a query. \n",
    "\n",
    "We store all the queries in a `dict` indexed by `q_id`s, similar to the following:\n",
    "```\n",
    "{\n",
    "    'q1'  : ['similar', 'law', ..., 'speed', 'aircraft'],\n",
    "    'q2'  : ['structur', 'aeroelast', ..., 'speed', 'aircraft'], \n",
    "    ...\n",
    "    'q225': ['design', 'factor', ..., 'number', '5']\n",
    "}\n",
    "```\n",
    "\n",
    "Again, please make sure the queries are inserted in ascending order of the query numbers as shown above. \n",
    "\n",
    "**This function should return a `dict` of length 225, where each key is a `str` representing a `q_id` and each value is a `list` that contains the tokens resulting from processing a query as described above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca1d6ae4b033997cc872d140096f86bc",
     "grade": false,
     "grade_id": "cell-7f0151dfa926854c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def load_cranfield_queries():\n",
    "    \"\"\"\n",
    "    Load and process cranfield queries\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    queries = {}\n",
    "    with open('assets/cranfield/cranfield.queries','r') as file:\n",
    "        for line in file:\n",
    "            toks = word_tokenize(line)\n",
    "            cont = [stemmer.stem(tok) for tok in toks if tok not in stop_words]\n",
    "            queries['q' + cont[0]] = cont[1:]\n",
    "\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8902565811364423a72a3ca51867e3",
     "grade": true,
     "grade_id": "cell-42d2a0ef1789f0a5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_queries = load_cranfield_queries()\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_queries, dict), \"Q1b: Your function should return a dictionary. \"\n",
    "assert len(stu_queries) == 225, \"Q1b: Your dictionary should contain 225 queries. \"\n",
    "\n",
    "for idx, q_id in enumerate(stu_queries, 1):\n",
    "    assert f\"q{idx}\" == q_id, f\"Q1b: {q_id} was not inserted in the correct order. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d6e9d2f6714777d3209c6aeb7198699",
     "grade": false,
     "grade_id": "cell-829314b779659ad7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 1c: Load the relevance judgements (10 pts)\n",
    "\n",
    "Finally, let's create a function for loading the relevance judgements from `assets/cranfield/cranfield.reljudge`. Each line of the file contains two whitespace-separated numbers representing a query number and a document number. For example, the first few lines read:\n",
    "\n",
    "```\n",
    "1 184\n",
    "1 29\n",
    "1 31\n",
    "```\n",
    "\n",
    "which, in our notations, can be interpreted as\n",
    "\n",
    "```\n",
    "d184 is relevant to q1\n",
    "d29  is relevant to q1\n",
    "d31  is relevant to q1\n",
    "```\n",
    "\n",
    "A query could have any number of relevant documents (or none). We store all the relevance judgements in a `dict` indexed by `q_id`s, similar to:\n",
    "```\n",
    "{\n",
    "    'q1': ['d184', 'd29', ..., 'd879', 'd880'],\n",
    "    'q2': ['d12', 'd15', ..., 'd864', 'd658'], \n",
    "    ...\n",
    "    'q225': ['d1379', 'd1305', ..., 'd1075', 'd1213']\n",
    "}\n",
    "```\n",
    "\n",
    "Likewise, please make sure the relevance judgements are inserted in ascending order of the query numbers as shown above. \n",
    "\n",
    "\n",
    "**This function should return a `dict` of length 225, where each key is a `str` representing a `q_id` and each value is a `list` that contains the `doc_id` of the relevant documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbded9f40317b300082f3678e70e9fb4",
     "grade": false,
     "grade_id": "cell-cb13d8eff761bf61",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_cranfield_reljudges():\n",
    "    \"\"\"\n",
    "    Load and process cranfield relevance judgements\n",
    "    \"\"\"\n",
    "    reljudges = {}\n",
    "    with open('assets/cranfield/cranfield.reljudge','r') as file:\n",
    "        for line in file:\n",
    "            if line.split()[0] not in reljudges:\n",
    "                reljudges[line.split()[0]] = [line.split()[1]]\n",
    "            else:\n",
    "                reljudges[line.split()[0]].append(line.split()[1])\n",
    "    reljudges = {'q' + k:['d' + item for item in v] for k, v in sorted(reljudges.items(), key = lambda item : int(item[0]))}\n",
    "\n",
    "    \n",
    "    return reljudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffcf2f4174ca6d11771a81605881dc59",
     "grade": true,
     "grade_id": "cell-436f8cfe2405ffe8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_reljudges = load_cranfield_reljudges()\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_reljudges, dict), \"Q1c: Your function should return a dictionary. \"\n",
    "assert len(stu_reljudges) == 225, \"Q1c: Your dictionary should contain relevance judgements for 225 queries. \"\n",
    "\n",
    "for idx, q_id in enumerate(stu_reljudges, 1):\n",
    "    assert f\"q{idx}\" == q_id, f\"Q1c: {q_id} was not inserted in the correct order. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_reljudges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ead046997b6384c3e3ee8fe6d957ea43",
     "grade": false,
     "grade_id": "cell-902e515e7633ecbd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Build an inverted index (20 pts)\n",
    "\n",
    "Now we can build an inverted index out of the Cranfield collection. Our inverted index should be a `dict` that looks similar to the following:\n",
    "\n",
    "```\n",
    "{\n",
    "    'experiment': {'d1': [1, [0]], ..., 'd30': [2, [12, 40]], ..., 'd123': [3, [11, 45, 67]], ...}, \n",
    "    \n",
    "    'studi': {'d1': [1, [1]], 'd2': [2, [0, 36]], ..., 'd207': [3, [19, 44, 59]], ...}\n",
    "    \n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "where the keys of the dictionary are the *terms*. Each term is associated with a `dict` indexed by the `doc_id` of the documents which contain that term. Along with each `doc_id` we also store the frequency and the positions of the term in each document, similar to what is shown in the lecture slides. \n",
    "\n",
    "More concretely, the above example shows two terms, \"experiment\" and \"studi\". In the dictionary associated with the term \"experiment\", the item `'d1': [1, [0]]` means \n",
    "\n",
    "* `d1` is a document that contains the term \"experiment\"; and \n",
    "* the term \"experiment\" appears `1` time in `d1` at index `0`. \n",
    "\n",
    "Similarly, in the dictionary associated with the term \"studi\", the item `'d207': [3, [19, 44, 59]]` means:\n",
    "\n",
    "* `d207` is a document that contains the term \"studi\"; and \n",
    "* the term \"studi\" appears `3` times in `d207` at indices `19`, `44` and `59`. \n",
    "\n",
    "\n",
    "We also need to decide what tokens are considered as terms. Not all tokens are qualified as terms to be tracked in our inverted index, except for those \"significant\" ones that appear in a \"reasonable\" number of documents. If a token appears in only one document, it may well be just noise. However, we must also be cautious about setting too high a threshold to include enough terms for our retrieval task. \n",
    "\n",
    "Complete the function below to build such an inverted index. It accepts two arguments: `docs` is a collection of documents as we built in Q1a and `min_df` specifies the minimum number of documents a token must appear in (a.k.a document frequency, df) to be included in the inverted index as a term. For example, `min_df=1` means we include all tokens as terms, because every token must have appeared in at least one document. In other words, we only consider tokens whose document frequency is at least `min_df` as terms. \n",
    "\n",
    "\n",
    "**This function should return a `dict` representing the inverted index built out of `docs` as specified above, which is indexed by terms as determined by `min_df`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b031968437f839c4ec34a4e4063c9d3",
     "grade": false,
     "grade_id": "cell-be2a75b95ae4c066",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_inverted_index(docs, min_df=1):\n",
    "    \"\"\"\n",
    "    Take a collection of documents and build an inverted index\n",
    "    \"\"\"\n",
    "    inv_index = {}\n",
    "    for doc_id, tokens in stu_docs.items():\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token not in inv_index:\n",
    "                inv_index[token] = {}\n",
    "            if doc_id not in inv_index[token]:\n",
    "                inv_index[token][doc_id] = [0,[]]\n",
    "            inv_index[token][doc_id][0]+=1\n",
    "            inv_index[token][doc_id][1].append(pos)\n",
    "    inv_index = {k:v for k, v in inv_index.items() if len(v) >= min_df}\n",
    "    \n",
    "\n",
    "    \n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ed89606882fccd53a9f3f2a4746763",
     "grade": true,
     "grade_id": "cell-bd592f77659516e2",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=1)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_inv_index, dict), \"Q2: Your function should return a dictionary. \"\n",
    "assert stu_inv_index, \"Q2: Your inverted index is empty. \"\n",
    "\n",
    "# All tokens should be included as terms when min_df=1\n",
    "all_tokens = {t for tokens in stu_docs.values() for t in tokens}\n",
    "all_terms = set(stu_inv_index.keys())\n",
    "\n",
    "assert all_tokens == all_terms, \"Q2: When min_df = 1, all tokens should be included as terms. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, all_tokens, all_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_inverted_index(stu_docs, min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfeba3b3fb90cc4c179aeb6335968f79",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3: Retrieve and rank documents (20 pts)\n",
    "\n",
    "Now let's write a function for retrieving and ranking documents. We will just follow the simple heuristic introduced in the lecture example. That is, for each *term* in a given query (remember, not all tokens are terms. How do you tell whether a token is a term or not, when given an inverted index?), we accumulate the term frequencies for each document that contains the term. Having repeated this procedure for all terms, we rank the documents found in descending order of their total term frequencies. If two documents tie for their total term frequencies, the document with a *lower* document number should be ranked higher; for example, if `d12` and `d100` have the same total term frequences for a given query, `d12` should be ranked higher than `d100` because `12 < 100`. Refer to the lecture slides titled \"Ranking Documents: Example\" in the \"Inverted Index\" slide deck for more details, where we have a short query \"data science\". \n",
    "\n",
    "Complete the function below to implement such a heuristic. The function takes as inputs an `inverted_index` as returned by the function in Q2, and some `queries` as returned by the function in Q1b. It also accepts an argument,  `max_docs`, to limit the number of documents retrieved to be at most `max_docs`. A special case is when `max_docs=-1`, where no upper limits on the number of documents retrieved should be imposed (i.e., just return all documents matched). The function should output a `dict` that contains an ordered `list` of retrieved documents for each query, similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q1'  : ['d51', 'd874', ..., 'd717'], \n",
    "    'q2'  : ['d51', 'd1147', ..., 'd14'],\n",
    "    ...,\n",
    "    'q225': ['d1313', 'd996', ..., 'd193']\n",
    "}\n",
    "```\n",
    "\n",
    "**This function should return a `dict` of length `len(queries)`, where each key is a `str` representing a `q_id` and each value is a `list` that contains the `doc_id` of the retrieved documents in order. `max_docs` determines the maximum number of documents to retrieve for each query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47e6b6cfeec3042e5410282431a33683",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_n_rank_docs(inverted_index, queries, max_docs=-1):\n",
    "    \"\"\"\n",
    "    Retrieve documents in order of relevance from an inverted index based on some queries\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for q_id, terms in queries.items():\n",
    "        res_q = {}\n",
    "        for term in terms:\n",
    "            if term in inverted_index:\n",
    "                for doc_id, val in inverted_index[term].items():\n",
    "                    if doc_id not in res_q:\n",
    "                        res_q[doc_id] = 0\n",
    "                    res_q[doc_id]+= val[0]\n",
    "\n",
    "        if max_docs == -1:\n",
    "            res[q_id] = [doc_id for doc_id, _ in sorted(res_q.items(), key = lambda x: (-x[1],int(x[0][1:])))]\n",
    "        else:\n",
    "            res[q_id] = [doc_id for doc_id, _ in sorted(res_q.items(), key = lambda x: (-x[1],int(x[0][1:])))][:max_docs] \n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0c8ef0374f3c47dfab47c90e7a0e597",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "min_df = 10 # min_df won't change in the hidden tests\n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=min_df)\n",
    "stu_queries = load_cranfield_queries()\n",
    "\n",
    "max_docs = 10 # max_docs may vary in the hidden tests\n",
    "stu_ret_docs = retrieve_n_rank_docs(stu_inv_index, stu_queries, max_docs=max_docs)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_ret_docs, dict), \"Q3: Your function should return a dictionary. \"\n",
    "assert len(stu_ret_docs) == len(stu_queries), \"Q3: Your dictionary should have the same length as there are queries. \"\n",
    "\n",
    "for q_id in stu_ret_docs:\n",
    "    \n",
    "    assert q_id in stu_queries, f\"Q3: When max_docs = {max_docs}, '{q_id}' in your dictionary is not a valid q_id. \"\n",
    "    \n",
    "    assert len(stu_ret_docs[q_id]) <= max_docs, f\"Q3: When max_docs = {max_docs}, your # retrieved docs ({len(stu_ret_docs[q_id])}) for {q_id} is bigger than max_docs. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, stu_queries, stu_ret_docs, min_df, max_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d26bae687249bcdc2d55515659a104ff",
     "grade": false,
     "grade_id": "cell-d49d380527e43db4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4: Evaluate your retrieval system (30 pts)\n",
    "\n",
    "Is your mini retrieval system doing a good job at all? Now let's evaluate it against some common metrics covered in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9520e90a3721dcce752febaf9177674",
     "grade": false,
     "grade_id": "cell-42457b95d61516c2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 4a: Calculate precision and recall @ $n$ (10 pts)\n",
    "\n",
    "Complete the function below for calculating precision and recall @ $n$. It takes as inputs a set of retrieved documents, `ret_docs`, as returned by the function in Q3, and a set of relevance judgements, `reljudges`, as returned by the function in Q1c, plus, of course, the argument `n`. **A special case arises when `n=-1` or when `n` exceeds the number of documents retrieved, where we calculate precision and recall based on all documents retrieved.** For example, if 10 documents were retrieved for a query, then `precision @ 12` is essentially the same as `precision @ 10` (and `precision @ -1` by our rules). The function should output two `dict`s containing the precision and recall @ `n` for each query in `ret_docs`, respectively, both similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q1'  : 0.1,\n",
    "    'q2'  : 0.3,\n",
    "    ...,\n",
    "    'q225': 0.2\n",
    "}\n",
    "```\n",
    "\n",
    "**This function should return two `dict`s both of length `len(ret_docs)`, where each key is a `str` representing a `q_id` and each value is a `float` representing either precision or recall @ `n`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "357f3dc339e50e789ccb383e2f53777e",
     "grade": false,
     "grade_id": "cell-a8150e5145aa8eed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_pre_rec_at_n(ret_docs, reljudges, n=-1):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall at n for each query in ret_docs\n",
    "    \"\"\"\n",
    "    pr_at_n = {}\n",
    "    rc_at_n = {}\n",
    "    \n",
    "    for query in ret_docs.keys():\n",
    "        effective_n = min(n,len(ret_docs[query])) if n != -1 else len(ret_docs[query])\n",
    "        counter = 0\n",
    "        for doc in ret_docs[query][:effective_n]:\n",
    "            if doc in reljudges[query]:\n",
    "                counter+=1\n",
    "        pr_at_n[query] = counter/len(ret_docs[query][:effective_n])\n",
    "        rc_at_n[query] = counter/len(reljudges[query])\n",
    " \n",
    "    return pr_at_n, rc_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37d321756212f463d34305e011363ee4",
     "grade": true,
     "grade_id": "cell-ce481913f67a4bf0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "import math\n",
    "\n",
    "min_df = 10 # min_df won't change in the hidden tests \n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=min_df)\n",
    "stu_queries = load_cranfield_queries()\n",
    "stu_reljudges = load_cranfield_reljudges()\n",
    "\n",
    "max_docs = 10 # max_docs may vary in the hidden tests\n",
    "stu_ret_docs = retrieve_n_rank_docs(stu_inv_index, stu_queries, max_docs=max_docs)\n",
    "\n",
    "stu_pre_at_n, stu_rec_at_n = calc_pre_rec_at_n(stu_ret_docs, stu_reljudges, n=-1)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_pre_at_n, dict), \"Q4a: Your function should return a dictionary for pre_at_n. \"\n",
    "assert isinstance(stu_rec_at_n, dict), \"Q4a: Your function should return a dictionary for rec_at_n. \"\n",
    "assert len(stu_pre_at_n) == len(stu_ret_docs), \"Q4a: Your pre_at_n should have the same length as there are queries. \"\n",
    "assert len(stu_rec_at_n) == len(stu_ret_docs), \"Q4a: Your rec_at_n should have the same length as there are queries. \"\n",
    "assert stu_pre_at_n.keys() == stu_ret_docs.keys(), \"Q4a: Your pre_at_n contains invalid queries. \"\n",
    "assert stu_rec_at_n.keys() == stu_ret_docs.keys(), \"Q4a: Your rec_at_n contains invalid queries. \"\n",
    "\n",
    "# Precision and recall must be between 0 and 1\n",
    "for q_id in stu_ret_docs:\n",
    "    assert 0.0 <= stu_pre_at_n[q_id] <= 1.0, f\"Q4a: Your precision @ n for {q_id} should be between 0 and 1. \"\n",
    "    assert 0.0 <= stu_rec_at_n[q_id] <= 1.0, f\"Q4a: Your recall @ n for {q_id} should be between 0 and 1. \"\n",
    "\n",
    "# Check it is still correct even if n exceeds max_docs\n",
    "stu_pre_at_n_max, stu_rec_at_n_max = calc_pre_rec_at_n(stu_ret_docs, stu_reljudges, n=(max_docs + 2))\n",
    "\n",
    "assert all(map(math.isclose, stu_pre_at_n.values(), stu_pre_at_n_max.values())), \"Q4a: When n exceeds max_docs, your function should return the same pre_at_n as when n=-1. \"\n",
    "assert all(map(math.isclose, stu_rec_at_n.values(), stu_rec_at_n_max.values())), \"Q4a: When n exceeds max_docs, your function should return the same rec_at_n as when n=-1. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, stu_queries, stu_reljudges, stu_ret_docs\n",
    "del stu_pre_at_n, stu_rec_at_n, stu_pre_at_n_max, stu_rec_at_n_max\n",
    "del min_df, max_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ea39bcc146683441aec091e2e0e03dc",
     "grade": false,
     "grade_id": "cell-aefd31eaf2760cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 4b: Calculate (mean) average precision (10 pts)\n",
    "\n",
    "Another commonly used metric is (mean) average precision. Complete the function below to calculate the average precision for each query and the mean average precision for all queries. It accepts exactly the same arguments as the `calc_pre_rec_at_n` function does in Q4a, except that the argument `n` is now renamed to `cutoff`. But otherwise `cutoff` serves a similar purpose to that of `n`: **only documents ranked higher than or at `cutoff` are considered as \"retrieved\"**. Similarly, a special case arises when `cutoff=-1` or when `cutoff` exceeds the number of documents retrieved, where we consider all documents as retrieved. The function should output a `dict` representing the average precisions for each query and a `float` representing the mean average precision. The `dict` should look similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q1'  : 0.03571428571428571,\n",
    "    'q2'  : 0.08194444444444444,\n",
    "    ...,\n",
    "    'q225': 0.02023809523809524\n",
    "}\n",
    "```\n",
    "\n",
    "**This function should return a `dict` of length `len(ret_docs)`, where each key is a `str` representing a `q_id` and each value is a `float` representing the average precision, and a `float` representing the mean average precision.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "574211fe57363b262296748f339b7844",
     "grade": false,
     "grade_id": "cell-6eeaf0c5eb760504",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_avg_pre(ret_docs, reljudges, cutoff=-1):\n",
    "    \"\"\"\n",
    "    Calculate (mean) average precision for each query in ret_docs\n",
    "    \"\"\"\n",
    "    avg_pre, mean_avg_pre = {},None\n",
    "    for query in ret_docs.keys():\n",
    "        counter = 0\n",
    "        effective_cutoff = min(cutoff,len(ret_docs[query])) if cutoff != -1 else len(ret_docs[query])\n",
    "        precision = []\n",
    "        \n",
    "        for i, doc in enumerate(ret_docs[query][:effective_cutoff]):\n",
    "            if doc in reljudges[query]:\n",
    "                counter += 1\n",
    "                precision.append(counter/(i+1))\n",
    "        avg_pre[query] = sum(precision)/len(reljudges[query])\n",
    "    mean_avg_pre = sum(avg_pre.values())/len(avg_pre.keys())\n",
    "    \n",
    "    return avg_pre, mean_avg_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3e21c1da95a44ce15c7ef3b0e5ad8f3",
     "grade": true,
     "grade_id": "cell-94b2a07cd98ffa25",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "import math\n",
    "\n",
    "min_df = 10 # min_df won't change in the hidden tests \n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=min_df)\n",
    "stu_queries = load_cranfield_queries()\n",
    "stu_reljudges = load_cranfield_reljudges()\n",
    "\n",
    "max_docs = 10 # max_docs may vary in the hidden tests\n",
    "stu_ret_docs = retrieve_n_rank_docs(stu_inv_index, stu_queries, max_docs=max_docs)\n",
    "\n",
    "stu_avg_pre, stu_mean_avg_pre = calc_avg_pre(stu_ret_docs, stu_reljudges, cutoff=-1)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_avg_pre, dict), \"Q4b: Your function should return a dictionary for avg_pre. \"\n",
    "assert isinstance(stu_mean_avg_pre, float), \"Q4b: Your function should return a float for mean_avg_pre. \"\n",
    "assert len(stu_avg_pre) == len(stu_ret_docs), \"Q4b: Your avg_pre should have the same length as there are queries. \"\n",
    "assert stu_avg_pre.keys() == stu_ret_docs.keys(), \"Q4b: Your avg_pre contains invalid queries. \"\n",
    "\n",
    "# Average precision must be between 0 and 1\n",
    "for q_id in stu_avg_pre:\n",
    "    assert 0.0 <= stu_avg_pre[q_id] <= 1.0, f\"Q4b: Your average precision for {q_id} should be between 0 and 1. \"\n",
    "\n",
    "# Check it is still correct even if cutoff exceeds max_docs\n",
    "stu_avg_pre_max, stu_mean_avg_pre_max = calc_avg_pre(stu_ret_docs, stu_reljudges, cutoff=(max_docs + 2))\n",
    "\n",
    "assert all(map(math.isclose, stu_avg_pre.values(), stu_avg_pre_max.values())), \"Q4b: When cutoff exceeds max_docs, your function should return the same results as when cutoff=-1. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, stu_queries, stu_reljudges, stu_ret_docs\n",
    "del stu_avg_pre, stu_mean_avg_pre, stu_avg_pre_max, stu_mean_avg_pre_max\n",
    "del min_df, max_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edd4561fdf2fea35779fadb904054fe9",
     "grade": false,
     "grade_id": "cell-1d52bb6b29966e5c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Question 4c: Calculate (normalised) discounted cumulative gain (DCG) (10 pts)\n",
    "\n",
    "Finally, let's try to evaluate our text retrieval system against normalised DCG (NDCG). The Cranfield collection by itself doesn't actually allow us to calculate NDCG, because NDCG requires a numerical relevance score for each document rather than just a binary relevance judgement as provided in the Cranfield collection. We circumvent this limitation by making the following assumptions.\n",
    "\n",
    "For a given query, \n",
    "\n",
    "* any documents not in `reljudges` (i.e., those irrelevant documents) have a relevance score of 1; and\n",
    "\n",
    "* relevant documents are assigned decreasing relevance scores in the order they appear in `reljudges`. For example, consider the following relevance judgements:\n",
    "\n",
    "```\n",
    "{\n",
    "    'q1': ['d184', 'd29', 'd879', 'd880'],\n",
    "    'q2': ['d12', 'd15', 'd658'] \n",
    "}\n",
    "```\n",
    "where we assign the following relevance scores to the relevant documents of each query:\n",
    "```\n",
    "{\n",
    "    'q1': [5, 4, 3, 2],\n",
    "    'q2': [4, 3, 2] \n",
    "}\n",
    "```\n",
    "In other words, the relevant document `d184` of `q1` is assigned a relevance score of 5, and the relevant document `d658` of `q2` is assigned a relevance score of 2. The *last* relevant document always gets a relevance score of 2, the *second last* relevant document always gets a relevance score of 3, and so on, until the first relevant document. \n",
    "The relevance scores increase by 1 each time we move away from the last relevant document, starting at 2.\n",
    "\n",
    "Complete the function below for calculating NDCG under the above assumptions. It accepts the usual arguments such as `ret_docs`, `reljudges`, and `n`. Not surprisingly, when `n=-1` or when `n` exceeds the number of documents retrieved, we calculate NDCG based on all documents retrieved. The additional argument `base` specifies the base in the logarithm function we use to discount relevance scores. The function outputs a `dict` containing the NDCG of each query, similar to:\n",
    "```\n",
    "{\n",
    "    'q1': 0.21628669853396418,\n",
    "    'q2': 0.3984097639816684,\n",
    "    ...,\n",
    "    'q225': 0.1361909750034715\n",
    "}\n",
    "```\n",
    "\n",
    "An important corner case arises when `n` exceeds the number of *relevant* documents. What should the ideal ranking be in that case? The top few documents in the ideal ranking are certainly the relevant documents in descending order of their relevant scores. But what documents should follow after the list of relevant documents is exhausted? What should their relevance scores be? \n",
    "\n",
    "**This function should return a `dict` of length `len(ret_docs)`, where each key is a `str` representing a `q_id` and each value is a `float` representing the NDCG.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13da0f6ca39bb494fc73f0288eb90681",
     "grade": false,
     "grade_id": "cell-ead65d17c71d2397",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def calc_NDCG_at_n(ret_docs, reljudges, n=-1, base=2):\n",
    "    ndcg = {}\n",
    "    for query in ret_docs.keys():\n",
    "        actual_n = min(n, len(ret_docs[query])) if n != -1 else len(ret_docs[query])\n",
    "        ret = ret_docs[query][:actual_n]\n",
    "        judge = reljudges[query]\n",
    "        judge_scores = {doc_id: len(judge) + 1 - i for i, doc_id in enumerate(judge)}\n",
    "        dcg = 0\n",
    "        for i, doc_id in enumerate(ret):\n",
    "            rel_score = judge_scores.get(doc_id,1)\n",
    "            dcg += rel_score if i < base else rel_score / math.log(i + 1, base)\n",
    "        ideal_dcg = 0\n",
    "        sorted_scores = sorted(judge_scores.values(), reverse=True) + [1] * (actual_n - len(judge))\n",
    "        for i, rel_score in enumerate(sorted_scores[:actual_n]):\n",
    "            ideal_dcg += rel_score if i < base else rel_score / math.log(i + 1, base)\n",
    "        ndcg[query] = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "    return ndcg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80865e3241b23bf781f59c1534170487",
     "grade": true,
     "grade_id": "cell-97527c3c993bc9a6",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "import math\n",
    "\n",
    "min_df = 10 # min_df won't change in the hidden tests \n",
    "stu_inv_index = build_inverted_index(stu_docs, min_df=min_df)\n",
    "stu_queries = load_cranfield_queries()\n",
    "stu_reljudges = load_cranfield_reljudges()\n",
    "\n",
    "max_docs = 10 # max_docs may vary in the hidden tests\n",
    "stu_ret_docs = retrieve_n_rank_docs(stu_inv_index, stu_queries, max_docs=max_docs)\n",
    "\n",
    "base = 2 # base may vary in the hidden tests\n",
    "stu_ndcg = calc_NDCG_at_n(stu_ret_docs, stu_reljudges, n=-1, base=base)\n",
    "\n",
    "# Some sanity checks\n",
    "assert isinstance(stu_ndcg, dict), \"Q4c: Your function should return a dictionary. \"\n",
    "assert len(stu_ndcg) == len(stu_ret_docs), \"Q4c: Your ndcg should have the same length as there are queries. \"\n",
    "assert stu_ndcg.keys() == stu_ret_docs.keys(), \"Q4c: Your ndcg contains invalid queries. \"\n",
    "\n",
    "# NDCG must be between 0 and 1\n",
    "for q_id in stu_ndcg:\n",
    "    assert 0.0 <= stu_ndcg[q_id] <= 1.0, f\"Q4c: Your NDCG for {q_id} should be between 0 and 1. \"\n",
    "\n",
    "# Check it is still correct even if n exceeds max_docs\n",
    "stu_ndcg_max = calc_NDCG_at_n(stu_ret_docs, stu_reljudges, n=(max_docs + 2), base=base)\n",
    "\n",
    "assert all(map(math.isclose, stu_ndcg.values(), stu_ndcg_max.values())), \"Q4c: When n exceeds max_docs, your function should return the same results as when n=-1. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_inv_index, stu_queries, stu_reljudges, stu_ret_docs\n",
    "del stu_ndcg, stu_ndcg_max\n",
    "del min_df, max_docs, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
